\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}

% Title Configuration
\title{\textbf{Analytics Copilot: Ask Questions About Your Data} \\ 
\large CS 5542 Big Data Analytics \& Applications - Project Proposal \\
\large \url{https://github.com/ben-blake/analytics-copilot}}
\author{\textbf{Ben Blake} (GenAI \& Backend Lead) \\ 
\textbf{Tina Nguyen} (Data \& Frontend Lead)}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This proposal outlines the design and implementation of an \textbf{Analytics Copilot}, an AI-powered system that democratizes data access by allowing non-technical users to query enterprise databases using natural language. Leveraging \textbf{Snowflake} as the core data engine and \textbf{Retrieval-Augmented Generation (RAG)} for schema context, the system translates plain English questions into executable SQL queries. By integrating recent advancements from NeurIPS 2025 such as schema abstraction and expert schema linking, our solution addresses the common "bottleneck" problem in data analytics where business users rely heavily on technical analysts. The project will feature a \textbf{Streamlit} interface for interaction, \textbf{Snowflake Cortex} for LLM inference, and a rigorous evaluation framework using synthetic "Golden Q\&A" pairs.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
The rapid advancement of Large Language Models (LLMs) has revolutionized how humans interact with digital systems. However, in the domain of enterprise data analytics, a significant gap remains. While LLMs excel at generating code, they often struggle with the specific nuances of enterprise database schemas, leading to "hallucinations"—plausible but incorrect SQL queries. For non-technical business users, this unreliability is a critical barrier to adoption.

This project proposes an \textbf{Analytics Copilot}, a robust, Snowflake-native system designed to bridge this gap. By leveraging a Retrieval-Augmented Generation (RAG) architecture specialized for structural metadata, our system allows users to ask questions in plain English and receive accurate, executable SQL queries and visualizations. Unlike generic chatbots, our solution is grounded in the specific schema of the organization, utilizing advanced techniques like \textit{schema linking} and \textit{semantic abstraction} to ensure high fidelity.

We chose \textbf{Snowflake} as our platform because of its integrated AI capabilities (Cortex), ensuring that data never leaves the secure governance boundary of the warehouse. This proposal outlines the architectural design, implementation strategy, and evaluation methodology for building a system that transforms the "Text-to-SQL" academic problem into a viable business utility.

\section{Objectives}

\subsection{Real-World Problem}
In modern data-driven organizations, the volume of data is exploding, but the ability to extract insights remains concentrated in the hands of a few technical experts (Data Analysts and Data Scientists). Business stakeholders—marketing managers, operations leads, and executives—often have critical questions but lack the SQL skills to answer them directly. This creates a significant bottleneck: simple questions ("What were total sales last Q4?") can take days to answer as they sit in ticket queues, delaying decision-making and reducing agility. Furthermore, when stakeholders attempt to use generic LLMs (like ChatGPT) for this task, they cannot easily provide the model with the secure access to the full database schema required for accurate answers.

\subsection{Target Users}
\begin{itemize}
    \item \textbf{Primary Users: Business Stakeholders.} These are domain experts (e.g., a Regional Sales Manager) who understand the \textit{business logic} but not the \textit{relational logic}. They need immediate answers to ad-hoc questions to inform strategy.
    \item \textbf{Secondary Users: Junior Data Analysts.} Analysts can use the Copilot to "jumpstart" their work, having the system draft complex joins or boilerplate SQL which they then refine, significantly increasing their productivity.
\end{itemize}

\subsection{Innovation Layer}
Our project goes beyond standard "Text-to-SQL" tutorials by implementing a multi-agentic architecture inspired by recent NeurIPS 2025 research. The core innovations include:
\begin{enumerate}
    \item \textbf{Semantic Metadata Layer (MAIA Adaptation):} We do not just pass raw table definitions (DDL) to the LLM, which are often cryptic (e.g., \texttt{t\_ord\_dt}). Instead, we implement a "Semantic View" layer in Snowflake that maps technical columns to verbose business concepts (e.g., "Transaction Date"), significantly reducing semantic ambiguity.
    \item \textbf{RAG-based Schema Linking (X-SQL Adaptation):} A common failure mode for Text-to-SQL is "context window overload," where feeding an LLM 100 table definitions causes it to get confused. We use a dedicated "Schema Linker" agent powered by Vector Search to dynamically retrieve only the top 3-5 relevant tables for a specific question.
    \item \textbf{Self-Correction Loop:} We implement a "Validation Agent" that runs a `EXPLAIN` plan on the generated SQL within Snowflake. If the database returns a syntax error (e.g., "Column not found"), the agent captures the error, feeds it back to the LLM, and requests a correction—transparently to the user.
\end{enumerate}

\section{Related Work (NeurIPS 2025)}
Our system design is directly informed by three cutting-edge papers from NeurIPS 2025 that address key challenges in Text-to-SQL.

\subsection{Chatting With Your Data (MAIA)}
\textbf{Paper:} \textit{Chatting With Your Data: LLM-Enabled Data Transformation for Enterprise Text to SQL} \citep{liu2025maia}. \\
\textbf{Summary:} This paper introduces MAIA, a framework that solves the "schema gap" by transforming rigid, normalized database schemas into a semantic layer that aligns with how humans think and speak. The authors argue that the primary cause of Text-to-SQL failure is not the LLM's reasoning ability, but the mismatch between "User Speak" and "Database Speak." \\
\textbf{Project Integration:} We adopt this philosophy by building a dedicated `METADATA` schema in Snowflake. For every table in our Olist dataset, we will generate (using a helper LLM) a JSON-based description that includes:
\begin{itemize}
    \item A natural language description of the table's purpose.
    \item Synonyms for column names (e.g., \texttt{zip\_code\_prefix} $\rightarrow$ "Postal Code", "Zip").
    \item Categorical value examples (e.g., "Payment types can be: credit\_card, boleto, voucher").
\end{itemize}
This metadata serves as the retrieval context for our RAG system, rather than the raw DDL.

\subsection{OmniSQL: Synthetic Data at Scale}
\textbf{Paper:} \textit{OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale} \citep{tang2024omnisql}. \\
\textbf{Summary:} OmniSQL demonstrates that high-quality synthetic training data can outperform human-annotated data. They generated 2.5 million text-SQL pairs to robustly train models, showing that diversity in SQL complexity (nested queries, window functions) is key to generalization. \\
\textbf{Project Integration:} We will use a similar synthetic generation pipeline to create our \textbf{Evaluation Dataset}. We will write a script that iterates through our schema and prompts an LLM to "imagine 5 hard business questions a CEO would ask about this data." We will then verify the SQL for these questions manually. This results in a "Golden Dataset" of ~50-100 questions that we use to quantitatively measure our system's accuracy, ensuring we aren't just relying on anecdotal "it looks good" testing.

\subsection{X-SQL: Expert Schema Linking}
\textbf{Paper:} \textit{X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs} \citep{wang2025xsql}. \\
\textbf{Summary:} X-SQL decomposes the problem into specialized agents: one for finding the right tables ("Schema Linking") and one for writing the query. This "divide and conquer" approach reduces hallucinations by preventing the SQL-writing LLM from seeing irrelevant table noise. \\
\textbf{Project Integration:} Our architecture explicitly separates the "retrieval" step from the "generation" step. We will index our table metadata into a **Snowflake Cortex Vector Search** index. When a query arrives, Step 1 is "Find relevant tables." Only the output of this step (e.g., `orders`, `customers`) is passed to Step 2 "Generate SQL." This aligns perfectly with the X-SQL findings that focused context improves accuracy.

\section{Data Sources}
We have selected datasets that represent realistic enterprise complexity (Relational Schemas) and standard benchmarks.

\subsection{Primary: Brazilian E-Commerce (Olist)}
\textbf{Source:} Kaggle (\url{https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce}) \\
\textbf{Description:} A rich relational dataset containing 100k orders from 2016-2018. It features 9 connected tables: \texttt{orders}, \texttt{customers}, \texttt{order\_items}, \texttt{payments}, \texttt{products}, \texttt{sellers}, \texttt{geolocation}, \texttt{product\_category\_name\_translation}, and \texttt{reviews}. \\
\textbf{Justification:} This dataset is ideal because it is \textit{highly normalized}. To answer a question like "What is the average review score for 'Health \& Beauty' products sold in 2017?", the system must join at least 4 tables (\texttt{reviews}, \texttt{orders}, \texttt{order\_items}, \texttt{products}). This complexity creates a genuine need for an AI assistant, as writing such a query manually is tedious even for experts.
\\
\textbf{Snowflake Ingestion Strategy:}
\begin{itemize}
    \item \textbf{Stage:} Upload CSVs to Snowflake Internal Stage (`@OLIST\_STAGE`).
    \item \textbf{Load:} Use `COPY INTO` commands to populate `RAW` schema tables.
    \item \textbf{Data Quality Checks:} Before analysis, we implement a \textbf{Validation Step} using Snowflake \textbf{Data Metric Functions}. We automatically check for:
    \begin{itemize}
        \item Null values in critical columns (e.g., \texttt{order\_id}, \texttt{price}).
        \item Referential integrity violations (e.g., an \texttt{order\_item} referencing a non-existent \texttt{product\_id}).
        \item Time range anomalies (e.g., orders dated in the future).
    \end{itemize}
    This ensures our AI doesn't learn from bad data.
    \item \textbf{Modeling:} We will strictly enforce Primary Keys (e.g., \texttt{order\_id}) and Foreign Keys in our definition DDL. While Snowflake doesn't enforce these constraints at write time, defining them provides critical hints to the Cortex LLM about how tables relate.
\end{itemize}

\subsection{Baseline: Superstore Sales}
\textbf{Source:} Kaggle (\url{https://www.kaggle.com/datasets/vivek468/superstore-dataset-final}) \\
\textbf{Description:} A single-table dataset (denormalized) representing retail sales. \\
\textbf{Usage:} Used for initial "Hello World" testing of the pipeline to verify latency and UI responsiveness without join complexity.

\subsection{Reference: Spider (Yale)}
\textbf{Source:} Hugging Face (\url{https://huggingface.co/datasets/xlangai/spider}) \\
\textbf{Description:} A massive cross-domain text-to-SQL dataset. \\
\textbf{Usage:} We will index complex SQL patterns from Spider (e.g., nested subqueries, `HAVING`, `CASE WHEN`) into our Vector Store. These serve as "Few-Shot Examples" retrieved at runtime to teach the LLM how to handle complex logic.

\section{Methods, Technologies \& Tools}

\subsection{System Architecture}
The system is built entirely around the \textbf{Snowflake Data Cloud} to ensure security, scalability, and unified governance.

\begin{figure}[H]
    \centering
    % Placeholder for the mermaid diagram we designed. 
    % In a real LaTeX doc, we would export the Mermaid to PNG and include it here.
    \includegraphics[width=\textwidth]{architecture.png}
    \caption{System Architecture: From User Query to SQL Execution}
    \label{fig:arch}
\end{figure}

The pipeline follows a sequential "Agentic" flow:
\begin{enumerate}
    \item \textbf{User Input:} The user asks a question via the Streamlit UI.
    \item \textbf{Context Retrieval (RAG):} The system embeds the user's question and searches the Vector Store for (a) relevant tables and (b) similar "Golden Queries" from our training set.
    \item \textbf{Prompt Assembly:} A dynamic prompt is constructed including the user question, the filtered schema definitions, and the few-shot examples.
    \item \textbf{SQL Generation (Cortex):} The prompt is sent to Snowflake Cortex (running Llama 3 70B or Mistral Large).
    \item \textbf{Validation (Self-Correction):} The generated SQL is dry-run using \texttt{EXPLAIN}. If valid, it executes. If invalid, the error is fed back to the LLM for a retry (max 3 retries).
    \item \textbf{Visualization:} The result set is returned to Streamlit, which automatically selects the best chart type (Bar, Line, Scatter) based on the data types (e.g., TimeSeries vs Categorical).
\end{enumerate}

\subsubsection{Detailed Agent Design}
To achieve high accuracy, we move beyond a simple "Chain" (A $\rightarrow$ B $\rightarrow$ C) to an "Agentic" workflow where steps can loop or branch.
\begin{itemize}
    \item \textbf{Schema Linker Agent:} This agent receives the user query and performs a \textit{hybrid search} (Keyword + Semantic) against the Vector Store. It returns a list of candidate tables. Crucially, it filters out tables with high semantic overlap but low relevance (e.g., `geolocation` is often retrieved but rarely needed for sales totals).
    \item \textbf{SQL Generator Agent:} This agent acts as the "Coder." It is prompted with a specific "Role" (e.g., "You are a Senior Snowflake Data Engineer") and provided with the \textit{Semantic Views} of the linked tables. It is explicitly instructed to use Snowflake-specific syntax (e.g., \texttt{TO\_VARCHAR}, \texttt{DATE\_TRUNC}).
    \item \textbf{Validation Agent:} This agent acts as the "Reviewer." It takes the generated SQL and runs a \texttt{EXPLAIN} query. If the \texttt{EXPLAIN} fails, it parses the error message and instructs the SQL Generator to fix it (e.g., "Error: Column 'profit' does not exist. Did you mean to calculate it as 'price - cost'?").
\end{itemize}

\subsection{Data Ingestion \& Storage}
We use a standard \textbf{ELT (Extract, Load, Transform)} pattern:
\begin{enumerate}
    \item \textbf{Ingest:} Data is uploaded to Snowflake Internal Stages.
    \item \textbf{Load:} `COPY INTO` moves data to `RAW` schema tables.
    \item \textbf{Transform:} Snowflake \textbf{Tasks} and \textbf{Streams} (optional) clean data types and create a `SILVER` layer.
    \item \textbf{Metadata:} A specialized `METADATA` table stores column descriptions and "Golden Queries" for RAG.
\end{enumerate}

\subsection{Analytics, ML \& GenAI}
\begin{itemize}
    \item \textbf{Vector Search:} We use \textbf{Snowflake Cortex Search} (or a vector-enabled table with \texttt{VECTOR} data type) to store embeddings of table schemas. This allows semantic search (e.g., searching for "revenue" matches the \texttt{payment\_value} column description).
    \item \textbf{LLM:} We utilize \textbf{Snowflake Cortex} (accessing models like Llama 3 or Mistral) for secure, serverless inference inside the data boundary.
    \item \textbf{Orchestration:} A Python-based controller (using LangChain or simple function calls) manages the flow. We specifically use \textbf{Snowflake Notebooks} as the development environment for these orchestration scripts, allowing us to version control the logic directly in git while executing close to the data.
    \item \textbf{Cortex Functions:} We leverage specific Snowflake Cortex functions such as \texttt{COMPLETE} (for text generation) and \texttt{EMBED\_TEXT\_768} (for creating vector embeddings of our schema descriptions). By using these built-in primitives, we avoid the complexity of managing external API keys and data egress fees.
\end{itemize}

\subsection{User Interface Design}
The frontend is built using \textbf{Streamlit}, chosen for its native integration with Snowflake and rapid prototyping capabilities. The UI features:
\begin{itemize}
    \item \textbf{Chat Interface:} A familiar "ChatGPT-style" message history where users can ask follow-up questions (e.g., "Now filter that by Q3").
    \item \textbf{Transparency Toggle:} An "Explain Logic" button that, when clicked, reveals the intermediate steps: the retrieved tables, the raw generated SQL, and the execution plan. This builds trust with technical users.
    \item \textbf{Visualization Library:} We use `altair` for declarative statistical visualization. The system heuristically maps query results to chart types:
    \begin{itemize}
        \item Time series data (Date + Metric) $\rightarrow$ Line Chart.
        \item Categorical comparisons (Category + Metric) $\rightarrow$ Bar Chart.
        \item Geographic data (Lat/Lon + Metric) $\rightarrow$ Map (using `pydeck`).
    \end{itemize}
\end{itemize}

\subsection{Infrastructure \& Deployment}
The entire application stack is designed to be deployed within the Snowflake environment:
\begin{itemize}
    \item \textbf{Frontend:} \textbf{Streamlit in Snowflake (SiS)}. This allows us to host the UI directly inside the data warehouse, meaning the data never travels over the public internet, a crucial feature for enterprise security.
    \item \textbf{Compute:} We utilize standard Snowflake Virtual Warehouses (XS size for development, S/M for demo) to handle the SQL execution load.
\end{itemize}

\section{Implementation Plan}
We have structured the project into four one-week sprints:

\begin{itemize}
    \item \textbf{Week 1: Data \& Infrastructure.} 
    \begin{itemize}
        \item Setup Snowflake account and GitHub repo.
        \item Ingest Olist and Superstore datasets.
        \item Define "Semantic Views" and write table descriptions.
    \end{itemize}
    \item \textbf{Week 2: RAG Pipeline Development.}
    \begin{itemize}
        \item Implement Vector Store for schema metadata.
        \item Develop the "Schema Linker" agent.
        \item Build the basic Text-to-SQL function using Cortex.
    \end{itemize}
    \item \textbf{Week 3: Frontend \& Refinement.}
    \begin{itemize}
        \item Build Streamlit interface.
        \item Implement the "Self-Correction" loop with `EXPLAIN` logic.
        \item Integrate visualization logic.
    \end{itemize}
    \item \textbf{Week 4: Evaluation \& Final Report.}
    \begin{itemize}
        \item Generate "Golden Dataset" using OmniSQL method.
        \item Run full evaluation benchmarks.
        \item Finalize PDF proposal and documentation.
    \end{itemize}
\end{itemize}

\section{Risk Analysis}
\begin{itemize}
    \item \textbf{Risk: Hallucination.} The LLM might invent columns that don't exist.
    \item \textbf{Mitigation:} Our "Schema Linking" step reduces the search space, and the "Self-Correction" agent catches syntax errors before the user sees them.
    
    \item \textbf{Risk: Latency.} Multi-step RAG chains (Retrieve $\rightarrow$ Generate $\rightarrow$ Validate) can be slow (10s+).
    \item \textbf{Mitigation:} We will optimize by using smaller, faster models (e.g., Mistral 7B) for the simple Schema Linking step, reserving the large model (Llama 3 70B) only for the complex SQL generation.
\end{itemize}

\section{Expected Outcomes}

\subsection{Live Demo}
Our final presentation will demonstrate a live session where we:
\begin{enumerate}
    \item Upload a new slice of data (e.g., "2025 Sales").
    \item Ask complex questions: "Show me the top 5 product categories by revenue in São Paulo."
    \item Watch the system \textit{think} (show retrieved tables), \textit{generate} SQL, and \textit{render} a bar chart.
    \item Demonstrate "Self-Correction" by asking a vague question and seeing the system ask for clarification or fix a syntax error.
\end{enumerate}

\subsection{Evaluation Metrics}
We will evaluate the system quantitatively:
\begin{itemize}
    \item \textbf{Execution Accuracy (EX):} Percentage of generated SQL queries that run without error and return the correct result set (compared to Golden SQL).
    \item \textbf{Logical Accuracy:} We will manually audit a sample of 20 queries to ensure the join logic (e.g., \texttt{INNER} vs \texttt{LEFT} join) is semantically correct for the business question.
    \item \textbf{Latency:} End-to-end time from "Submit" to "Chart Rendered."
    \item \textbf{Token Efficiency:} Cost per query based on prompt size (reduced by our Schema Linking strategy).
\end{itemize}

\subsection{Evaluation Methodology: The Golden Dataset}
To rigorously measure "Execution Accuracy," we cannot rely on ad-hoc testing. We will construct a \textbf{Golden Dataset} of 50-100 Question-SQL pairs derived from the Olist schema. This process involves:
\begin{enumerate}
    \item \textbf{Synthetic Generation:} Using the \textit{OmniSQL} methodology, we prompt an LLM to generate diverse questions ranging from easy ("Count orders") to hard ("Average delivery time by state").
    \item \textbf{Human Verification:} We manually review the generated SQL for correctness, correcting any logical errors (e.g., ensuring \texttt{freight\_value} is summed correctly).
    \item \textbf{Automated Benchmarking:} We write a Python script that runs all 100 questions through our Copilot, captures the generated SQL, executes it, and compares the result set (row counts, values) against the Golden SQL results. This provides a hard metric (e.g., "82\% Accuracy") that we can track as we improve the prompts.
\end{enumerate}

\section{Final Deliverables}
\begin{enumerate}
    \item \textbf{GitHub Repository:} A public repo containing all source code (`/src`), documentation (`/docs`), and setup scripts (`/reproducibility`): \url{https://github.com/ben-blake/analytics-copilot}
    \item \textbf{Prototype App:} A functional Streamlit web application.
    \item \textbf{Snowpark Notebooks:} Python notebooks demonstrating the data ingestion and vector indexing steps.
    \item \textbf{Final Report:} A comprehensive PDF detailing our design decisions, challenges, and evaluation results.
\end{enumerate}


\newpage
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
