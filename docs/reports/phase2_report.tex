\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multirow}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!8},
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{orange},
}

% Title Configuration
\title{\textbf{Analytics Copilot: Ask Questions About Your Data} \\
\large Phase 2 Report: Foundations \& Reproducibility \\
\large CS 5542 Big Data Analytics \& Applications \\
\large \url{https://github.com/ben-blake/analytics-copilot}}
\author{\textbf{Ben Blake} (GenAI \& Backend Lead) \\
\textbf{Tina Nguyen} (Data \& Frontend Lead)}
\date{February 27, 2026}

\begin{document}

\maketitle

\begin{abstract}
This report documents the Phase 2 implementation of the \textbf{Analytics Copilot}, a fully reproducible, end-to-end system that translates natural language questions into executable Snowflake SQL queries. The system integrates a three-stage agentic pipeline—Schema Linker (RAG), SQL Generator (Cortex LLM), and Validator (self-correction)—deployed as a Streamlit chat interface backed by the Olist Brazilian E-Commerce dataset hosted in Snowflake. Key Phase 2 deliverables include: ingestion of all 9 Olist relational tables into a \texttt{RAW} schema, a semantic metadata layer in a \texttt{METADATA} schema with Cortex-generated column descriptions, a Cortex Search retrieval service, an evaluation framework of 50 golden queries, and a reproducibility guide validated end-to-end. Execution accuracy on the golden query benchmark currently stands at \textbf{$\sim$72--80\%} and improves measurably with each prompt engineering iteration.
\end{abstract}

\tableofcontents
\newpage

% ─────────────────────────────────────────────────────────
\section{Dataset \& Knowledge Base Documentation}
% ─────────────────────────────────────────────────────────

\subsection{Primary Dataset: Olist Brazilian E-Commerce}

\begin{itemize}[noitemsep]
    \item \textbf{Name:} Brazilian E-Commerce Public Dataset by Olist
    \item \textbf{Source:} Kaggle — \url{https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce}
    \item \textbf{Modality:} Tabular (CSV), relational schema
    \item \textbf{Domain:} E-commerce order management, logistics, and customer reviews
    \item \textbf{Scale:} $\sim$100{,}000 orders, 2016--2018, across 9 CSV files
\end{itemize}

The dataset is a \textit{highly normalized} relational schema that requires multi-table JOINs to answer realistic business questions (e.g.\ answering ``What is the average review score for health \& beauty products?''\ requires joining four tables). This complexity makes it an ideal benchmark for a Text-to-SQL system.

\subsection{Table Schema}

All nine CSVs are ingested into the \texttt{ANALYTICS\_COPILOT.RAW} schema under clean, abbreviated table names:

\begin{table}[H]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{CSV File} & \textbf{Snowflake Table} & \textbf{Row Count} \\
\midrule
\texttt{olist\_orders\_dataset.csv} & \texttt{RAW.ORDERS} & 99{,}441 \\
\texttt{olist\_customers\_dataset.csv} & \texttt{RAW.CUSTOMERS} & 99{,}441 \\
\texttt{olist\_order\_items\_dataset.csv} & \texttt{RAW.ORDER\_ITEMS} & 112{,}650 \\
\texttt{olist\_order\_payments\_dataset.csv} & \texttt{RAW.ORDER\_PAYMENTS} & 103{,}886 \\
\texttt{olist\_order\_reviews\_dataset.csv} & \texttt{RAW.ORDER\_REVIEWS} & 99{,}224 \\
\texttt{olist\_products\_dataset.csv} & \texttt{RAW.PRODUCTS} & 32{,}951 \\
\texttt{olist\_sellers\_dataset.csv} & \texttt{RAW.SELLERS} & 3{,}095 \\
\texttt{olist\_geolocation\_dataset.csv} & \texttt{RAW.GEOLOCATION} & 1{,}000{,}163 \\
\texttt{product\_category\_name\_translation.csv} & \texttt{RAW.PRODUCT\_CATEGORY\_TRANSLATION} & 71 \\
\bottomrule
\end{tabular}
\caption{Olist CSV to Snowflake table mappings}
\end{table}

\subsection{Semantic Metadata Layer (Knowledge Base)}

Rather than feeding raw DDL to the LLM—a known cause of hallucination—we construct a \textbf{Semantic Metadata Layer} in the \texttt{ANALYTICS\_COPILOT.METADATA} schema. The \texttt{TABLE\_DESCRIPTIONS} table stores one row per column, containing:

\begin{itemize}[noitemsep]
    \item \texttt{table\_name}, \texttt{column\_name}, \texttt{data\_type}
    \item \texttt{description}: business-friendly natural language description (LLM-generated)
    \item \texttt{synonyms}: alternative user-facing names (e.g., \textit{payment\_value} $\to$ ``revenue, sales, order total'')
    \item \texttt{sample\_values}: representative values for categorical columns
\end{itemize}

This metadata is generated by \texttt{scripts/build\_metadata.py}, which calls \texttt{SNOWFLAKE.\allowbreak CORTEX.\allowbreak COMPLETE('llama3.1-70b')} to produce descriptions for every column in the \texttt{RAW} schema.

\subsection{Data Preprocessing}

No statistical sampling was required (the full dataset is used). Preprocessing steps applied:
\begin{enumerate}[noitemsep]
    \item \textbf{Stage Upload:} CSVs are uploaded to a Snowflake Internal Stage (\texttt{@OLIST\_STAGE}) via Python Snowpark \texttt{session.file.put()}.
    \item \textbf{COPY INTO:} Data is loaded using \texttt{COPY INTO} with \texttt{FILE\_FORMAT = CSV\_FORMAT} (header skip, UTF-8 encoding, null-if-empty).
    \item \textbf{Type coercion:} Date columns (\texttt{order\_purchase\_timestamp}, etc.) are cast to \texttt{TIMESTAMP\_NTZ}; numeric columns are cast to \texttt{NUMBER(10,2)}.
    \item \textbf{Key constraints:} Primary and foreign keys are declared in DDL (\path{snowflake/02_olist_tables.sql}) to provide JOIN hints to the LLM, even though Snowflake does not enforce them at write time.
\end{enumerate}

% ─────────────────────────────────────────────────────────
\section{Retrieval \& Processing Pipeline}
% ─────────────────────────────────────────────────────────

\subsection{Architecture Overview}

The Schema Linker agent implements a three-level fallback retrieval chain:

\begin{enumerate}[noitemsep]
    \item \textbf{Cortex Search} (semantic, primary): \texttt{SNOWFLAKE.\allowbreak CORTEX.\allowbreak SEARCH\_PREVIEW} over \texttt{TABLE\_DESCRIPTIONS}
    \item \textbf{Keyword ILIKE} (lexical, fallback): \texttt{ILIKE} matching on column names, descriptions, and synonyms
    \item \textbf{Full table dump} (last resort): returns all \texttt{RAW} tables from \texttt{INFORMATION\_SCHEMA}
\end{enumerate}

\subsection{Chunking Strategy}

The retrieval corpus is \textit{column-level chunks}—each row in \texttt{TABLE\_DESCRIPTIONS} represents one column and is the atomic unit of retrieval. This granularity allows the system to precisely identify which columns are relevant to a question, rather than retrieving entire table definitions.

After retrieval, column-level results are \textbf{grouped by \texttt{table\_name}} and aggregated with an average relevance score. Only the top-$k$ tables (default $k=5$) are passed to the SQL Generator, ensuring the prompt context window contains only relevant schema.

\subsection{Indexing \& Retrieval Configuration}

\begin{itemize}[noitemsep]
    \item \textbf{Service:} \texttt{ANALYTICS\_COPILOT.\allowbreak METADATA.\allowbreak SCHEMA\_SEARCH\_SERVICE} (Snowflake Cortex Search)
    \item \textbf{Source table:} \texttt{METADATA.TABLE\_DESCRIPTIONS}
    \item \textbf{Search column:} \texttt{description} (primary embedding target)
    \item \textbf{Return columns:} \texttt{table\_name}, \texttt{column\_name}, \texttt{description}, \texttt{synonyms}, \texttt{data\_type}, \texttt{sample\_values}
    \item \textbf{Retrieval type:} Dense semantic search (embeddings managed internally by Cortex Search)
    \item \textbf{FK supplementation:} When anchor tables are retrieved without their join partners (e.g., \texttt{ORDER\_ITEMS} without \texttt{ORDERS}), the system automatically appends the missing partner tables based on a hardcoded FK graph
\end{itemize}

\subsection{Example Retrieval Outputs}

\textbf{Query 1:} \textit{``What is the average delivery time by customer state?''}

\begin{lstlisting}[language=SQL, caption=Schema Linker output for delivery time query]
-- Tables returned (ranked by relevance):
-- 1. ORDERS (score: 0.87)
--    Columns: ORDER_PURCHASE_TIMESTAMP, ORDER_DELIVERED_CUSTOMER_DATE,
--             ORDER_ESTIMATED_DELIVERY_DATE, ORDER_STATUS, CUSTOMER_ID
-- 2. CUSTOMERS (score: 0.72)  [supplemented via FK: ORDERS -> CUSTOMERS]
--    Columns: CUSTOMER_ID, CUSTOMER_UNIQUE_ID, CUSTOMER_STATE, CUSTOMER_CITY
\end{lstlisting}

\textbf{Query 2:} \textit{``Show me total revenue by product category''}

\begin{lstlisting}[language=SQL, caption=Schema Linker output for revenue by category query]
-- Tables returned (ranked by relevance):
-- 1. ORDER_ITEMS (score: 0.91)
--    Columns: ORDER_ID, PRODUCT_ID, SELLER_ID, PRICE, FREIGHT_VALUE
-- 2. PRODUCTS (score: 0.83)
--    Columns: PRODUCT_ID, PRODUCT_CATEGORY_NAME, PRODUCT_DESCRIPTION_LENGHT
-- 3. PRODUCT_CATEGORY_TRANSLATION (score: 0.61)
--    Columns: PRODUCT_CATEGORY_NAME, PRODUCT_CATEGORY_NAME_ENGLISH
\end{lstlisting}

% ─────────────────────────────────────────────────────────
\section{Application Integration}
% ─────────────────────────────────────────────────────────

\subsection{Streamlit Chat Interface}

The application (\texttt{src/app.py}) is a Streamlit chat interface that exposes the full three-agent pipeline to end users. Key UI components:

\begin{itemize}[noitemsep]
    \item \textbf{Chat history:} Persistent multi-turn conversation with labeled user/assistant bubbles
    \item \textbf{Status indicators:} Live progress steps shown during processing (``Finding relevant tables\ldots'', ``Generating SQL\ldots'', ``Validating\ldots'')
    \item \textbf{SQL expander:} Collapsed ``Show SQL'' block reveals the final executed query
    \item \textbf{Results table:} Pandas DataFrame rendered via \texttt{st.dataframe} (capped at 1{,}000 display rows to prevent browser lag)
    \item \textbf{Auto-visualization:} \texttt{src/utils/viz.py} heuristically selects chart type based on column types: line chart (datetime + numeric), bar chart (categorical + numeric), scatter plot (2 numeric columns)
\end{itemize}

\subsection{Grounded Answer Generation}

The SQL Generator uses \texttt{SNOWFLAKE.\allowbreak CORTEX.\allowbreak COMPLETE('llama3.1-70b')} with a structured prompt that includes:
\begin{enumerate}[noitemsep]
    \item \textbf{System role:} ``You are a Senior Snowflake Data Engineer\ldots''
    \item \textbf{Dataset context:} Olist schema summary and FK relationship map
    \item \textbf{Critical rules:} 15 explicit constraints (fully qualified table names, Snowflake \texttt{DATEDIFF} syntax, no invented tables, LIMIT defaults for superlative queries, etc.)
    \item \textbf{Available tables list:} Dynamically injected from schema linker output
    \item \textbf{Few-shot examples:} Two built-in examples covering CTE patterns for top-N-per-group and month-over-month LAG calculations
    \item \textbf{Dynamic examples:} Up to 3 golden examples retrieved from \texttt{data/golden\_queries.json}
\end{enumerate}

\subsection{Query Logging \& Evaluation}

The evaluation framework (\texttt{scripts/evaluate.py}) runs all 50 golden queries through the full pipeline and reports:

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Result} \\
\midrule
Execution Accuracy (no SQL error) & $\sim$72--80\% \\
Easy queries (single-table) & $\sim$95\% \\
Medium queries (2--3 table JOINs) & $\sim$75\% \\
Hard queries (CTEs, window functions) & $\sim$55\% \\
Average end-to-end latency & $\sim$5--8s per query \\
\bottomrule
\end{tabular}
\caption{Evaluation results on 50 Olist golden queries}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{architecture.png}
    \caption{System architecture: Data Sources $\to$ Snowflake $\to$ Agent Pipeline $\to$ Streamlit UI}
\end{figure}

% ─────────────────────────────────────────────────────────
\section{Snowflake Data Pipeline \& Schema}
% ─────────────────────────────────────────────────────────

\subsection{Schema Layout}

The Snowflake database \texttt{ANALYTICS\_COPILOT} contains two schemas:

\begin{itemize}[noitemsep]
    \item \textbf{\texttt{RAW}:} 9 Olist tables + \texttt{SUPERSTORE\_SALES} (optional baseline)
    \item \textbf{\texttt{METADATA}:} \texttt{TABLE\_DESCRIPTIONS}, \texttt{COLUMN\_DESCRIPTIONS}, \texttt{SCHEMA\_SEARCH\_SERVICE} (Cortex Search)
\end{itemize}

\subsection{DDL Scripts}

Five sequential SQL scripts in \texttt{snowflake/} initialize the full environment:

\begin{enumerate}[noitemsep]
    \item \texttt{01\_setup.sql} — Database, schemas, warehouse, file format
    \item \texttt{02\_olist\_tables.sql} — 9 RAW tables with PK/FK constraints
    \item \texttt{03\_superstore.sql} — Optional SUPERSTORE\_SALES table
    \item \texttt{04\_metadata.sql} — \texttt{TABLE\_DESCRIPTIONS} and \texttt{COLUMN\_DESCRIPTIONS} tables
    \item \texttt{05\_cortex\_search.sql} — Cortex Search service over \texttt{TABLE\_DESCRIPTIONS}
\end{enumerate}

\subsection{Reproducible Ingestion}

Data ingestion is fully scripted via \texttt{scripts/ingest\_data.py} using the Snowflake Snowpark Python SDK. The script:
\begin{enumerate}[noitemsep]
    \item Executes DDL scripts in order
    \item Uploads each CSV to an internal stage via \texttt{session.file.put()}
    \item Loads data with \texttt{session.sql("COPY INTO \ldots")}
    \item Validates row counts against expected values
\end{enumerate}

\subsection{Example Warehouse Queries}

\begin{lstlisting}[language=SQL, caption=Example: Average delivery time by state (Snowflake syntax)]
SELECT
    ANALYTICS_COPILOT.RAW.CUSTOMERS.CUSTOMER_STATE,
    AVG(DATEDIFF('day',
        ANALYTICS_COPILOT.RAW.ORDERS.ORDER_PURCHASE_TIMESTAMP,
        ANALYTICS_COPILOT.RAW.ORDERS.ORDER_DELIVERED_CUSTOMER_DATE
    )) AS AVG_DELIVERY_DAYS
FROM ANALYTICS_COPILOT.RAW.ORDERS
JOIN ANALYTICS_COPILOT.RAW.CUSTOMERS
    ON ANALYTICS_COPILOT.RAW.ORDERS.CUSTOMER_ID
     = ANALYTICS_COPILOT.RAW.CUSTOMERS.CUSTOMER_ID
WHERE ORDER_DELIVERED_CUSTOMER_DATE IS NOT NULL
GROUP BY ANALYTICS_COPILOT.RAW.CUSTOMERS.CUSTOMER_STATE
ORDER BY AVG_DELIVERY_DAYS;
\end{lstlisting}

\begin{lstlisting}[language=SQL, caption=Example: Top product categories by revenue]
SELECT
    t.PRODUCT_CATEGORY_NAME_ENGLISH,
    SUM(oi.PRICE) AS TOTAL_REVENUE
FROM ANALYTICS_COPILOT.RAW.ORDER_ITEMS oi
JOIN ANALYTICS_COPILOT.RAW.PRODUCTS p
    ON oi.PRODUCT_ID = p.PRODUCT_ID
JOIN ANALYTICS_COPILOT.RAW.PRODUCT_CATEGORY_TRANSLATION t
    ON p.PRODUCT_CATEGORY_NAME = t.PRODUCT_CATEGORY_NAME
GROUP BY t.PRODUCT_CATEGORY_NAME_ENGLISH
ORDER BY TOTAL_REVENUE DESC
LIMIT 10;
\end{lstlisting}

\subsection{Snowflake--Application Integration}

The application connects to Snowflake via \path{src/utils/snowflake_conn.py}, which uses Snowflake Snowpark Python with credentials loaded from a \texttt{.env} file (RSA key-pair or password authentication). All LLM inference (\texttt{CORTEX.COMPLETE}), semantic search (\texttt{CORTEX.SEARCH\_PREVIEW}), and SQL execution happen inside Snowflake — no data leaves the warehouse boundary.

% ─────────────────────────────────────────────────────────
\section{Reproducibility Plan}
% ─────────────────────────────────────────────────────────

\subsection{Environment Configuration}

All Python dependencies are pinned in \texttt{requirements.txt} at the project root. Key packages:

\begin{lstlisting}
snowflake-snowpark-python>=1.21.0
streamlit>=1.32.0
altair>=5.2.0
pandas>=2.0.0
python-dotenv>=1.0.0
\end{lstlisting}

A \texttt{.env.example} file documents all required environment variables:
\begin{lstlisting}
SNOWFLAKE_ACCOUNT=your_account_identifier
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password          # or use PRIVATE_KEY_PATH
SNOWFLAKE_PRIVATE_KEY_PATH=./rsa_key.p8
SNOWFLAKE_ROLE=TRAINING_ROLE
SNOWFLAKE_WAREHOUSE=COPILOT_WH
SNOWFLAKE_DATABASE=ANALYTICS_COPILOT
SNOWFLAKE_SCHEMA=RAW
\end{lstlisting}

\subsection{Full Reproduction Steps}

\begin{enumerate}
    \item \textbf{Clone repository} and create a virtual environment:
\begin{lstlisting}[language=bash]
git clone https://github.com/ben-blake/analytics-copilot.git
cd analytics-copilot
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
\end{lstlisting}

    \item \textbf{Configure credentials}: Copy \texttt{.env.example} to \texttt{.env} and fill in Snowflake credentials.

    \item \textbf{Initialize Snowflake} (run once):
\begin{lstlisting}[language=bash]
snowsql -f snowflake/01_setup.sql
snowsql -f snowflake/02_olist_tables.sql
snowsql -f snowflake/04_metadata.sql
snowsql -f snowflake/05_cortex_search.sql
\end{lstlisting}

    \item \textbf{Download data}: Download the Olist dataset from Kaggle and unzip into \texttt{data/olist/}.

    \item \textbf{Ingest data}:
\begin{lstlisting}[language=bash]
python scripts/ingest_data.py
\end{lstlisting}

    \item \textbf{Build semantic metadata}:
\begin{lstlisting}[language=bash]
python scripts/build_metadata.py
\end{lstlisting}

    \item \textbf{Launch application}:
\begin{lstlisting}[language=bash]
streamlit run src/app.py
\end{lstlisting}

    \item \textbf{Run evaluation} (optional):
\begin{lstlisting}[language=bash]
python scripts/evaluate.py
\end{lstlisting}
\end{enumerate}

The full guide (with troubleshooting) is available in \texttt{reproducibility/README.md}.

\subsection{Model Versions \& Configuration}

\begin{itemize}[noitemsep]
    \item \textbf{LLM:} \texttt{llama3.1-70b} via \texttt{SNOWFLAKE.\allowbreak CORTEX.\allowbreak COMPLETE} (no local model weights — inference is serverless inside Snowflake)
    \item \textbf{Embeddings:} Managed internally by Cortex Search (no explicit embedding model selection required)
    \item \textbf{No random seeds required:} The system is deterministic — SQL generation is prompt-driven with no stochastic sampling parameters
    \item \textbf{Golden dataset:} Pre-generated and committed to \path{data/golden_queries.json} (50 queries across easy/medium/hard difficulty)
\end{itemize}

% ─────────────────────────────────────────────────────────
\section{GitHub Repository}
% ─────────────────────────────────────────────────────────

\textbf{URL:} \url{https://github.com/ben-blake/analytics-copilot}

The repository is structured as follows:

\begin{lstlisting}
analytics-copilot/
+-- src/
|   +-- agents/          # Schema Linker, SQL Generator, Validator
|   |   +-- schema_linker.py
|   |   +-- sql_generator.py
|   |   +-- validator.py
|   +-- utils/           # Snowflake connection, visualization
|   |   +-- snowflake_conn.py
|   |   +-- viz.py
|   +-- app.py           # Streamlit chat interface
+-- scripts/             # Data ingestion, metadata builder, evaluation
|   +-- ingest_data.py
|   +-- build_metadata.py
|   +-- generate_golden.py
|   +-- evaluate.py
+-- snowflake/           # SQL DDL scripts (setup, tables, Cortex Search)
|   +-- 01_setup.sql
|   +-- 02_olist_tables.sql
|   +-- 03_superstore.sql
|   +-- 04_metadata.sql
|   +-- 05_cortex_search.sql
+-- data/                # CSV data, golden queries, evaluation results
|   +-- olist/           # Brazilian E-Commerce CSVs
|   +-- superstore/      # Superstore Sales CSV (optional)
|   +-- golden_queries.json
+-- docs/                # Architecture diagrams and project reports
|   +-- architecture.png
|   +-- architecture.mmd
|   +-- reports/         # Phase reports and proposal (PDF + LaTeX)
|       +-- proposal.pdf
|       +-- proposal.tex
|       +-- phase2_report.pdf
|       +-- phase2_report.tex
+-- reproducibility/     # Setup instructions and troubleshooting guide
|   +-- README.md
+-- CONTRIBUTIONS.md
+-- requirements.txt
+-- .env.example
\end{lstlisting}

% ─────────────────────────────────────────────────────────
\section{Individual Contribution Statement}
% ─────────────────────────────────────────────────────────

\begin{table}[H]
\centering
\begin{tabular}{p{2.5cm}p{8.5cm}c}
\toprule
\textbf{Member} & \textbf{Contribution Description} & \textbf{\%} \\
\midrule
Ben Blake &
GenAI pipeline design and implementation: Schema Linker agent (RAG with Cortex Search, 3-level fallback, FK supplementation, stopword filtering), SQL Generator agent (prompt engineering, few-shot examples, 15-rule constraint system, CUSTOMER\_UNIQUE\_ID data model notes), Validator agent (EXPLAIN-based self-correction loop, 3-retry mechanism), evaluation framework (\texttt{scripts/evaluate.py}, 50-query golden dataset), Snowflake schema design (DDL scripts, PK/FK definitions, Cortex Search service), iterative accuracy improvements (0\% $\to$ 72--80\%).
& 50\% \\
\addlinespace
Tina Nguyen &
Data ingestion and preprocessing pipeline (\texttt{scripts/ingest\_data.py}, Snowpark \texttt{COPY INTO}), semantic metadata generation (\texttt{scripts/build\_metadata.py}, Cortex LLM descriptions), Streamlit UI (\texttt{src/app.py}, chat interface, status indicators, expanders, row-cap safety), auto-visualization logic (\texttt{src/utils/viz.py}, chart type heuristics, aggregate-column preference), Snowflake connection utility (\texttt{src/utils/snowflake\_conn.py}), documentation (\texttt{README.md}, \texttt{reproducibility/README.md}, \texttt{CONTRIBUTIONS.md}).
& 50\% \\
\bottomrule
\multicolumn{2}{r}{\textbf{Total}} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\caption{Individual contribution breakdown}
\end{table}

Contributions are supported by commit history at the GitHub repository linked above. The GenAI pipeline (Cortex integration, agents, RAG), Snowflake schema design, evaluation framework, and backend architecture was committed by Ben Blake; the Data ingestion \& preprocessing, Streamlit UI, visualization logic, metadata generation, and documentation were committed by Tina Nguyen.

\end{document}